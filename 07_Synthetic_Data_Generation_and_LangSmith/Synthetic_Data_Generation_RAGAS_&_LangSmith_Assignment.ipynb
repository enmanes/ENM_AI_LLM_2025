{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCk2Rx4cjlYF"
      },
      "source": [
        "# Synthetic Data Generation Using RAGAS - RAG Evaluation with LangSmith\n",
        "\n",
        "In the following notebook we'll explore a use-case for RAGAS' synthetic testset generation workflow!\n",
        "\n",
        "\n",
        "\n",
        "- 🤝 BREAKOUT ROOM #1\n",
        "  1. Use RAGAS to Generate Synthetic Data\n",
        "\n",
        "- 🤝 BREAKOUT ROOM #2\n",
        "  1. Load them into a LangSmith Dataset\n",
        "  2. Evaluate our RAG chain against the synthetic test data\n",
        "  3. Make changes to our pipeline\n",
        "  4. Evaluate the modified pipeline\n",
        "\n",
        "SDG is a critical piece of the puzzle, especially for early iteration! Without it, it would not be nearly as easy to get high quality early signal for our application's performance.\n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bG2ta-B478G"
      },
      "source": [
        "# 🤝 BREAKOUT ROOM #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VUI7vF_kbv9"
      },
      "source": [
        "## Task 1: Dependencies and API Keys\n",
        "\n",
        "We'll need to install a number of API keys and dependencies, since we'll be leveraging a number of great technologies for this pipeline!\n",
        "\n",
        "1. OpenAI's endpoints to handle the Synthetic Data Generation\n",
        "2. OpenAI's Endpoints for our RAG pipeline and LangSmith evaluation\n",
        "3. QDrant as our vectorstore\n",
        "4. LangSmith for our evaluation coordinator!\n",
        "\n",
        "Let's install and provide all the required information below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies and API Keys:\n",
        "\n",
        "> NOTE: DO NOT RUN THESE CELLS IF YOU ARE RUNNING THIS NOTEBOOK LOCALLY"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
=======
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/175.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.7/175.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m160.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "source": [
        "#!pip install -qU ragas==0.2.10"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 63,
=======
      "execution_count": null,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install -qU langchain-community==0.3.14 langchain-openai==0.2.14 unstructured==0.16.12 langgraph==0.2.61 langchain-qdrant==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also want to set a project name to make things easier for ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - SDG - {uuid4().hex[0:8]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenAI's API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Synthetic Test Data\n",
        "\n",
        "We wil be using Ragas to build out a set of synthetic test questions, references, and reference contexts. This is useful because it will allow us to find out how our system is performing.\n",
        "\n",
        "> NOTE: Ragas is best suited for finding *directional* changes in your LLM-based systems. The absolute scores aren't comparable in a vacuum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation\n",
        "\n",
        "We'll prepare our data - and download our webpages which we'll be using for our data today.\n",
        "\n",
        "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n",
        "\n",
        "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n",
        "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n",
        "\n",
        "Let's start by collecting our data into a useful pile!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 31287    0 31287    0     0  92995      0 --:--:-- --:--:-- --:--:-- 92839\n"
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 70146    0 70146    0     0   182k      0 --:--:-- --:--:-- --:--:--  183k\n"
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's load our data into a familiar LangChain format using the `DirectoryLoader`."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 4,
=======
      "execution_count": 7,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.html\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Knowledge Graph Based Synthetic Generation\n",
        "\n",
        "Ragas uses a knowledge graph based approach to create data. This is extremely useful as it allows us to create complex queries rather simply. The additional testset complexity allows us to evaluate larger problems more effectively, as systems tend to be very strong on simple evaluation tasks.\n",
        "\n",
        "Let's start by defining our `generator_llm` (which will generate our questions, summaries, and more), and our `generator_embeddings` which will be useful in building our graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unrolled SDG"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
=======
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jupiter-core/Code/AIMS/AIE5 Staging/07_Synthetic_Data_Generation_and_LangSmith/.venv/lib/python3.13/site-packages/pysbd/segmenter.py:66: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  for match in re.finditer('{0}\\s*'.format(re.escape(sent)), self.original_text):\n",
            "/home/jupiter-core/Code/AIMS/AIE5 Staging/07_Synthetic_Data_Generation_and_LangSmith/.venv/lib/python3.13/site-packages/pysbd/lang/arabic.py:29: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  txt = re.sub('(?<={0})\\.'.format(am), '∯', txt)\n",
            "/home/jupiter-core/Code/AIMS/AIE5 Staging/07_Synthetic_Data_Generation_and_LangSmith/.venv/lib/python3.13/site-packages/pysbd/lang/persian.py:29: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  txt = re.sub('(?<={0})\\.'.format(am), '∯', txt)\n"
          ]
        }
      ],
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
<<<<<<< HEAD
        "#ENM added the following 4 lines - all lines added to deal with API Retry Rate Issues\n",
        "from langchain.globals import set_llm_cache\n",
        "from langchain.cache import InMemoryCache\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "\n",
        "#generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(\n",
        "    ChatOpenAI(\n",
        "       # model=\"gpt-4\",\n",
        "        # model=\"gpt-3.5-turbo\",\n",
        "        model=\"gpt-4o\", #ENM ADDED THE REMAINING FOR Rate issues\n",
        "        temperature=0,\n",
        "        request_timeout=30,\n",
        "        max_retries=5\n",
        "        #time.sleep(2) \n",
        "        #retry_decorator=retry(\n",
        "        #    wait=wait_random_exponential(min=1, max=60),\n",
        "        #    stop=stop_after_attempt(3)\n",
        "        #)\n",
        "        #model_kwargs={\"retry_delay\":60}\n",
        "    )\n",
        ")\n",
        "\n",
        "#@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "#def completion_with_backoff(**kwargs):\n",
        "#    return openai.Completion.create(**kwargs)\n",
        "# completion_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n",
        "\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(\n",
        "    OpenAIEmbeddings(#ENM ADDED THE REMAINING FOR Rate issues\n",
        "        max_retries=5\n",
        "        #time.sleep(2) \n",
        "        #retry_decorator=retry(\n",
        "        #    wait=wait_random_exponential(min=1, max=60),\n",
        "        #    stop=stop_after_attempt(3)\n",
        "        #)\n",
        "    #model_kwargs={\"retry_delay\":60}\n",
        "    )\n",
        ")"
=======
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we're going to instantiate our Knowledge Graph.\n",
        "\n",
        "This graph will contain N number of nodes that have M number of relationships. These nodes and relationships (AKA \"edges\") will define our knowledge graph and be used later to construct relevant questions and responses."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 6,
=======
      "execution_count": 9,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 0, relationships: 0)"
            ]
          },
<<<<<<< HEAD
          "execution_count": 6,
=======
          "execution_count": 9,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.graph import KnowledgeGraph\n",
        "\n",
        "kg = KnowledgeGraph()\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step we're going to take is to simply insert each of our full documents into the graph. This will provide a base that we can apply transformations to."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 7,
=======
      "execution_count": 10,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 2, relationships: 0)"
            ]
          },
<<<<<<< HEAD
          "execution_count": 7,
=======
          "execution_count": 10,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.graph import Node, NodeType\n",
        "\n",
        "for doc in docs:\n",
        "    kg.nodes.append(\n",
        "        Node(\n",
        "            type=NodeType.DOCUMENT,\n",
        "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
        "        )\n",
        "    )\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we'll apply the *default* transformations to our knowledge graph. This will take the nodes currently on the graph and transform them based on a set of [default transformations](https://docs.ragas.io/en/latest/references/transforms/#ragas.testset.transforms.default_transforms).\n",
        "\n",
        "These default transformations are dependent on the corpus length, in our case:\n",
        "\n",
        "- Producing Summaries -> produces summaries of the documents\n",
        "- Extracting Headlines -> finding the overall headline for the document\n",
        "- Theme Extractor -> extracts broad themes about the documents\n",
        "\n",
        "It then uses cosine-similarity and heuristics between the embeddings of the above transformations to construct relationships between the nodes."
      ]
    },
    {
<<<<<<< HEAD
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENM NOTE 1:   SO FAR, 3/2/25 - NONE OF THE RATE LIMITERS WORKED - JUST CHANGED GPT MODEL TO 3.5-TURBO\n",
        "Ultimately - this didn't work either - needed to ask OpenAI why my access was stuck at free instead of TIER 1 and they claimed a \"bug\" which was covered in many Reddit posts. Once I was upgraded, the problems disappeared\n",
        "\n",
        "USE THE FOLLOWING TO TRY AND DEAL WITH THE RATE LIMIT ISSUES WITH OPENAI\n",
        "\n",
        "STILL RAN INTO ERRORS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import backoff\n",
        "from ragas.testset.transforms import default_transforms, apply_transforms\n",
        "from openai import OpenAIError  \n",
        "\n",
        "# Function to retry on rate limit errors using exponential backoff\n",
        "@backoff.on_exception(backoff.expo, OpenAIError , max_tries=7, max_time=120)\n",
        "def apply_transforms_with_retry(kg, transforms):\n",
        "    \"\"\"Applies transformations with a retry mechanism on rate-limit errors.\"\"\"\n",
        "    return apply_transforms(kg, transforms)\n",
        "\n",
        "# Ensure a delay before applying transformations (optional)\n",
        "time.sleep(20)\n",
        "\n",
        "# Define Transformer and Embedding Model\n",
        "transformer_llm = generator_llm\n",
        "embedding_model = generator_embeddings  # Ensure this is initialized correctly\n",
        "\n",
        "# Apply transformations with retry\n",
        "try:\n",
        "    default_transforms = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
        "    kg = apply_transforms_with_retry(kg, default_transforms)\n",
        "    print(\"Transformations applied successfully!\")\n",
        "except OpenAIError as e:\n",
        "    print(f\"Failed due to OpenAI API error: {e}\")\n",
        "\n",
        "kg  # Return or print the knowledge graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENM NOTE 2:\n",
        "\n",
        "TRYING THIS MORE DIRECT APPROACH TO EDITING BATCH SIZES AND OTHER THINGS - ALSO DID NOT WORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import backoff\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from ragas.testset.transforms import default_transforms, apply_transforms\n",
        "from openai import OpenAIError  # Correct import for OpenAI API errors\n",
        "from langchain_core.documents.base import Document\n",
        "\n",
        "# Use GPT-3.5-turbo for lower token costs (optional)\n",
        "#MODEL_NAME = \"gpt-3.5-turbo\"  # Change back to \"gpt-4o\" if needed\n",
        "MODEL_NAME = \"gpt-4o\"#gpt-3.5-turbo\"  # Change back to \"gpt-4o\" if needed\n",
        "\n",
        "# Initialize the LLM Wrapper\n",
        "generator_llm = LangchainLLMWrapper(\n",
        "    ChatOpenAI(\n",
        "        model=MODEL_NAME,\n",
        "        temperature=0,\n",
        "        request_timeout=30,\n",
        "        max_retries=5  # OpenAI's built-in retry mechanism\n",
        "    )\n",
        ")\n",
        "\n",
        "# Function to retry API calls when hitting rate limits\n",
        "@backoff.on_exception(backoff.expo, OpenAIError, max_tries=7, max_time=120)\n",
        "def apply_transforms_with_retry(kg, transforms):\n",
        "    \"\"\"Applies transformations with a retry mechanism on rate-limit errors.\"\"\"\n",
        "    return apply_transforms(kg, transforms)\n",
        "\n",
        "# **Step 1: Reduce Input Size Before Making API Calls**\n",
        "#def truncate_documents(docs, max_tokens=2500):\n",
        "#    \"\"\"Truncate documents to fit within token limits.\"\"\"\n",
        "#    truncated_docs = []\n",
        "#    for doc in docs:\n",
        "#        if len(doc) > max_tokens:\n",
        "#            truncated_docs.append(doc[:max_tokens])  # Keep first max_tokens characters\n",
        "#        else:\n",
        "#            truncated_docs.append(doc)\n",
        "#    return truncated_docs\n",
        "#\n",
        "#docs = truncate_documents(docs)  # Apply truncation to reduce size\n",
        "\n",
        "# Step 1: Truncate Documents\n",
        "truncated_texts = truncate_documents(docs)  # Get list of truncated strings\n",
        "\n",
        "# Step 2: Convert Truncated Strings Back into LangChain Document Objects\n",
        "truncated_docs = [Document(page_content=text) for text in truncated_texts]\n",
        "\n",
        "# Step 3: Apply Transforms with Correct Document Format\n",
        "default_transforms = default_transforms(documents=truncated_docs, llm=transformer_llm, embedding_model=embedding_model)\n",
        "\n",
        "BATCH_SIZE = 10  # Adjust based on API limits\n",
        "for i in range(0, len(default_transforms), BATCH_SIZE):\n",
        "    batch = default_transforms[i : i + BATCH_SIZE]\n",
        "    try:\n",
        "        kg = apply_transforms_with_retry(kg, batch)\n",
        "        time.sleep(60)  # Introduce delay between batch requests\n",
        "        print(f\"Processed batch {i//BATCH_SIZE + 1}/{(len(default_transforms) // BATCH_SIZE) + 1}\")\n",
        "    except OpenAIError as e:\n",
        "        print(f\"Failed due to OpenAI API error: {e}\")\n",
        "\n",
        "kg  # Return or print the knowledge graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENM Note3:  Another trial, many of these trials were attempts with Todd that just weren't working.  I would love to figure out a way to make them work even if it took forever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Define a function that adds a delay to a Completion API call\n",
        "def delayed_completion(delay_in_seconds: float = 1, **kwargs):\n",
        "    \"\"\"Delay a completion by a specified amount of time.\"\"\"\n",
        "\n",
        "    # Sleep for the delay\n",
        "    time.sleep(delay_in_seconds)\n",
        "\n",
        "    # Call the Completion API and return the result\n",
        "    return client.chat.completions.create(**kwargs)\n",
        "\n",
        "\n",
        "# Calculate the delay based on your rate limit\n",
        "rate_limit_per_minute = 3  # ENM'S CURRENT OPENAI RATE LIMIT IS 3PM\n",
        "# ENM-USING 55 sec TO ENSURE BELOW THE MINUTE RATE LIMIT\n",
        "delay = 55 / rate_limit_per_minute\n",
        "\n",
        "delayed_completion(\n",
        "    delay_in_seconds=delay,\n",
        "    #ALSO CONSIDER USING MODELS WITH HIGHER LIMITS\n",
        "    #model=\"gpt-3.5-turbo\",\n",
        "    model=\"gpt-4o\", \n",
        "    messages=[{\"role\": \"user\", \"content\": \"Once upon a time,\"}]\n",
        ")\n",
        "\n",
        "# THE FOLLOWING WAS SOME OTHER CODE THAT I TRIED\n",
        "'''\n",
        "import time\n",
        "import backoff\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from ragas.testset.transforms import default_transforms, apply_transforms\n",
        "from openai import OpenAIError  # Correct import for OpenAI API errors\n",
        "from langchain_core.documents.base import Document\n",
        "\n",
        "# Use GPT-3.5-turbo for lower token costs (optional)\n",
        "#MODEL_NAME = \"gpt-3.5-turbo\"  # Change back to \"gpt-4o\" if needed\n",
        "MODEL_NAME = \"gpt-4o\"#gpt-3.5-turbo\"  # Change back to \"gpt-4o\" if needed\n",
        "\n",
        "# Initialize the LLM Wrapper\n",
        "generator_llm = LangchainLLMWrapper(\n",
        "    ChatOpenAI(\n",
        "        model=MODEL_NAME,\n",
        "        temperature=0,\n",
        "        request_timeout=30,\n",
        "        max_retries=5  # OpenAI's built-in retry mechanism\n",
        "    )\n",
        ")\n",
        "\n",
        "# Function to retry API calls when hitting rate limits\n",
        "@backoff.on_exception(backoff.expo, OpenAIError, max_tries=7, max_time=120)\n",
        "def apply_transforms_with_retry(kg, transforms):\n",
        "    \"\"\"Applies transformations with a retry mechanism on rate-limit errors.\"\"\"\n",
        "    return apply_transforms(kg, transforms)\n",
        "\n",
        "# **Step 1: Reduce Input Size Before Making API Calls**\n",
        "#def truncate_documents(docs, max_tokens=2500):\n",
        "#    \"\"\"Truncate documents to fit within token limits.\"\"\"\n",
        "#    truncated_docs = []\n",
        "#    for doc in docs:\n",
        "#        if len(doc) > max_tokens:\n",
        "#            truncated_docs.append(doc[:max_tokens])  # Keep first max_tokens characters\n",
        "#        else:\n",
        "#            truncated_docs.append(doc)\n",
        "#    return truncated_docs\n",
        "#\n",
        "#docs = truncate_documents(docs)  # Apply truncation to reduce size\n",
        "\n",
        "# Step 1: Truncate Documents\n",
        "truncated_texts = truncate_documents(docs)  # Get list of truncated strings\n",
        "\n",
        "# Step 2: Convert Truncated Strings Back into LangChain Document Objects\n",
        "truncated_docs = [Document(page_content=text) for text in truncated_texts]\n",
        "\n",
        "# Step 3: Apply Transforms with Correct Document Format\n",
        "default_transforms = default_transforms(documents=truncated_docs, llm=transformer_llm, embedding_model=embedding_model)\n",
        "\n",
        "BATCH_SIZE = 10  # Adjust based on API limits\n",
        "for i in range(0, len(default_transforms), BATCH_SIZE):\n",
        "    batch = default_transforms[i : i + BATCH_SIZE]\n",
        "    try:\n",
        "        kg = apply_transforms_with_retry(kg, batch)\n",
        "        time.sleep(60)  # Introduce delay between batch requests\n",
        "        print(f\"Processed batch {i//BATCH_SIZE + 1}/{(len(default_transforms) // BATCH_SIZE) + 1}\")\n",
        "    except OpenAIError as e:\n",
        "        print(f\"Failed due to OpenAI API error: {e}\")\n",
        "\n",
        "kg  # Return or print the knowledge graph\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENM TRIAL 4: ALSO DIDN'T WORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import backoff\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from ragas.testset.transforms import default_transforms, apply_transforms\n",
        "from openai import OpenAIError\n",
        "\n",
        "# Initialize the LLM Wrapper\n",
        "generator_llm = LangchainLLMWrapper(\n",
        "    ChatOpenAI(\n",
        "        model=\"gpt-4o\",  # Consider using \"gpt-3.5-turbo\" for higher RPM limits\n",
        "        temperature=0,\n",
        "        request_timeout=30,\n",
        "        max_retries=5\n",
        "    )\n",
        ")\n",
        "\n",
        "# OpenAI rate limits\n",
        "MAX_REQUESTS_PER_MINUTE = 3  # Update this based on your OpenAI account limits\n",
        "DELAY_BETWEEN_REQUESTS = 65 / MAX_REQUESTS_PER_MINUTE  # Calculate safe delay\n",
        "\n",
        "# Function to retry API calls when hitting rate limits\n",
        "@backoff.on_exception(backoff.expo, OpenAIError, max_tries=5, max_time=300)\n",
        "def apply_transforms_with_retry(kg, batch):\n",
        "    \"\"\"Applies transformations with a retry mechanism on rate-limit errors.\"\"\"\n",
        "    return apply_transforms(kg, batch)\n",
        "\n",
        "# Convert truncated text back into LangChain Document format\n",
        "from langchain_core.documents.base import Document\n",
        "truncated_texts = truncate_documents(docs)\n",
        "truncated_docs = [Document(page_content=text) for text in truncated_texts]\n",
        "\n",
        "# Apply transforms in batches with proper delays\n",
        "BATCH_SIZE = 2  # Adjust based on API limits\n",
        "for i in range(0, len(truncated_docs), BATCH_SIZE):\n",
        "    batch = truncated_docs[i : i + BATCH_SIZE]\n",
        "    \n",
        "    try:\n",
        "        kg = apply_transforms_with_retry(kg, batch)\n",
        "        print(f\"Processed batch {i//BATCH_SIZE + 1}/{(len(truncated_docs) // BATCH_SIZE) + 1}\")\n",
        "        \n",
        "        # **Smart Delay to Avoid Rate Limits**\n",
        "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
        "\n",
        "    except OpenAIError as e:\n",
        "        print(f\"Failed due to OpenAI API error: {e}\")\n",
        "\n",
        "kg  # Return or print the knowledge graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENM TRIAL 5 - ALSO DIDN'T WORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import backoff\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from ragas.testset.transforms import default_transforms, apply_transforms\n",
        "from openai import OpenAIError\n",
        "from langchain_core.documents.base import Document\n",
        "\n",
        "# Initialize the LLM Wrapper\n",
        "generator_llm = LangchainLLMWrapper(\n",
        "    ChatOpenAI(\n",
        "        model=\"gpt-4o\",  # Consider using \"gpt-3.5-turbo\" for higher RPM limits\n",
        "        temperature=0,\n",
        "        request_timeout=30,\n",
        "        max_retries=3\n",
        "    )\n",
        ")\n",
        "\n",
        "# OpenAI rate limits\n",
        "MAX_REQUESTS_PER_MINUTE = 3  \n",
        "DELAY_BETWEEN_REQUESTS = 65 / MAX_REQUESTS_PER_MINUTE  \n",
        "\n",
        "# Function to retry API calls when hitting rate limits\n",
        "@backoff.on_exception(backoff.expo, OpenAIError, max_tries=5, max_time=300)\n",
        "def apply_transforms_with_retry(kg, batch):\n",
        "    \"\"\"Applies transformations with a retry mechanism on rate-limit errors.\"\"\"\n",
        "    return apply_transforms(kg, batch)\n",
        "\n",
        "# Convert LangChain Document objects into a format that `apply_transforms()` expects\n",
        "formatted_docs = [{\"text\": doc.page_content} for doc in truncated_docs]  \n",
        "\n",
        "# Apply default transforms correctly\n",
        "default_transforms = default_transforms(documents=formatted_docs, llm=generator_llm, embedding_model=embedding_model)\n",
        "\n",
        "# Apply transformations in batches with a delay to prevent rate limits\n",
        "BATCH_SIZE = 2  \n",
        "for i in range(0, len(default_transforms), BATCH_SIZE):\n",
        "    batch = default_transforms[i : i + BATCH_SIZE]\n",
        "    \n",
        "    try:\n",
        "        kg = apply_transforms_with_retry(kg, batch)\n",
        "        print(f\"Processed batch {i//BATCH_SIZE + 1}/{(len(default_transforms) // BATCH_SIZE) + 1}\")\n",
        "\n",
        "        # Smart delay to avoid rate limits\n",
        "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
        "\n",
        "    except OpenAIError as e:\n",
        "        print(f\"Failed due to OpenAI API error: {e}\")\n",
        "\n",
        "kg  # Return or print the knowledge graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENM NOTE ON DOCS - RECEIVED AN ERROR FROM CURSOR GPT THAT \"maybe\" THE ISSUE WAS THE DOCUMENT FOR FACTOR SO PRINTED IT OUT TO SEE THAT IT WAS OK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "[Document(metadata={}, page_content='Simon Willison’s Weblog\\n\\nSubscribe\\n\\nThings we learned about LLMs in 2024\\n\\n31st December 2024\\n\\nA lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\\n\\nThis is a sequel to my review of 2023.\\n\\nIn this article:\\n\\nThe GPT-4 barrier was comprehensively broken\\n\\nSome of those GPT-4 models run on my laptop\\n\\nLLM prices crashed, thanks to competition and increased efficiency\\n\\nMultimodal vision is common, audio and video are starting to emerge\\n\\nVoice and live camera mode are science fiction come to life\\n\\nPrompt driven app generation is a commodity already\\n\\nUniversal access to the best models lasted for just a few short months\\n\\n“Agents” still haven’t really happened yet\\n\\nEvals really matter\\n\\nApple Intelligence is bad, Apple’s MLX library is excellent\\n\\nThe rise of inference-scaling “reasoning” models\\n\\nWas the best currently available LLM trained in China for less than $6m?\\n\\nThe environmental impact got better\\n\\nThe environmental impact got much, much worse\\n\\nThe year of slop\\n\\nSynthetic training data works great\\n\\nLLMs somehow got even harder to use\\n\\nKnowledge is incredibly unevenly distributed\\n\\nLLMs need better criticism\\n\\nEverything tagged “llms” on my blog in 2024\\n\\nThe GPT-4 barrier was comprehensively broken\\n\\nIn my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t?\\n\\nI’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total.\\n\\nThe earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\\n\\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.\\n\\nGemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million.\\n\\nLonger inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.\\n\\nGetting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).\\n\\nThen there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\\n\\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list.\\n\\nSome of those GPT-4 models run on my laptop\\n\\nMy personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\\n\\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:\\n\\nQwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model!\\n\\nI can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December)\\n\\nThis remains astonishing to me. I thought')]\n"
          ]
        }
      ],
      "source": [
        "print(type(batch))  # Should be a list of documents or processed chunks\n",
        "print(batch[:2])  # Show the first 2 elements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENM NOTE 6: THIS WAS POSTED BY KAT ON DISCORD BUT UNSURE IF IT WAS FOR THIS ASSIGNMENT OR THE NEXT ONE(S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kat posted something in the Discord Discussions that had the following:\n",
        "from langchain.chat_models import init_chat_model \n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter \n",
        "rate_limiter = InMemoryRateLimiter( \n",
        "\trequests_per_second=1, # <-- make a request once every 1 seconds!! \n",
        "\tcheck_every_n_seconds=0.1, # Wake up every 100 ms to check whether allowed to make a request, max_bucket_size=10, # Controls the maximum burst size.\n",
        "\t )\n",
        " model = init_chat_model(\"gpt-4o\", temperature=0, rate_limiter=rate_limiter) \n",
        " generator_llm = LangchainLLMWrapper(model) \n",
        " generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings()) \n",
        "\n",
        "from ragas.testset import TestsetGenerator \n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings) \n",
        "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)\n",
        "\n",
        "# From <https://discord.com/channels/1135695983720792216/1327051742638116894/threads/1342710885277237268> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENM NOTE 7:  THIS WAS A TRIAL AS WELL BUT DIDN'T NEED TO USE IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# THIS WAS THE ORIGINAL CODE slightly modified THAT RAN INTO RATE LIMIT ERRORS LEADING TO THE CREATION OF THE CODES ABOVE\n",
        "\n",
        "from ragas.testset.transforms import default_transforms, apply_transforms\n",
        "from langchain.chat_models import init_chat_model \n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter \n",
        "rate_limiter = InMemoryRateLimiter( \n",
        "\trequests_per_second=0.045, # <-- make a request 2.7X/MIN!! \n",
        "\tcheck_every_n_seconds=0.1, # Wake up every 100 ms to check whether allowed to make a request, max_bucket_size=10, # Controls the maximum burst size.\n",
        "\t )\n",
        "\n",
        "model = init_chat_model(\"gpt-4o\", temperature=0, rate_limiter=rate_limiter) \n",
        "generator_llm = LangchainLLMWrapper(model) \n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings()) \n",
        "\n",
        "transformer_llm = generator_llm\n",
        "embedding_model = generator_embeddings\n",
        "\n",
        "default_transforms = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
        "apply_transforms(kg, default_transforms)\n",
        "kg\n",
        "\n",
        "'''\n",
        "# ALL OF THE FOLLOWING IS FROM KAT'S CODE - NEED TO ADJUST TO WORK WITH THE CODE ABOVE...\n",
        "from langchain.chat_models import init_chat_model \n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter \n",
        "rate_limiter = InMemoryRateLimiter( \n",
        "\trequests_per_second=1, # <-- make a request once every 1 seconds!! \n",
        "\tcheck_every_n_seconds=0.1, # Wake up every 100 ms to check whether allowed to make a request, max_bucket_size=10, # Controls the maximum burst size.\n",
        "\t )\n",
        " model = init_chat_model(\"gpt-4o\", temperature=0, rate_limiter=rate_limiter) \n",
        " generator_llm = LangchainLLMWrapper(model) \n",
        " generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings()) \n",
        "\n",
        "from ragas.testset import TestsetGenerator \n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings) \n",
        "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENM NOTE 8: THE FOLLOWING WAS THE ORIGINAL CODE WITH NO DELAYS\n",
        "\n",
        "SKIP THE FOLLOWING IF USING ANY OF THE FORMER NOTE/TRIALS;\n",
        "THIS ULTIMATELY WORKED AFTER OPENAI GOT ME TIER 1 ACCESS .... *sigh* BUT LOST 2 WEEKS TRYING TO GET AROUND RATELIMIT ERRORS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
=======
      "cell_type": "code",
      "execution_count": 11,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "a89ccfa8b6844afa8e6dcaad3d9300cd",
=======
              "model_id": "05af86dd82a94f96af764fbc18e736a6",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "32864065bc5c4db983a1eef2569664c1",
=======
              "model_id": "2f4585f1876c4dc88180ca4d030d5666",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "c28a3f8900c949b1aafe7f7232136e37",
=======
              "model_id": "6ca9bf184a1543648af3774766c6a8a1",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "90388bab59514b809419df0be2ca2476",
=======
              "model_id": "881dd76eed704bdf8e0bbfd16306e1cb",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "d9923ccce0994f8a9d53c77ccfd43987",
=======
              "model_id": "de74078c9d62495c92f4754fd12ca883",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/26 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "34793ca1ae5148cc8bec55dd9869b242",
=======
              "model_id": "66fac7ea08bb49e388a3782a994dd0c0",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
<<<<<<< HEAD
              "KnowledgeGraph(nodes: 14, relationships: 70)"
            ]
          },
          "execution_count": 8,
=======
              "KnowledgeGraph(nodes: 14, relationships: 67)"
            ]
          },
          "execution_count": 11,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
<<<<<<< HEAD
        "# ORIGINAL NOTEBOOK CODE\n",
=======
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
        "from ragas.testset.transforms import default_transforms, apply_transforms\n",
        "\n",
        "transformer_llm = generator_llm\n",
        "embedding_model = generator_embeddings\n",
        "\n",
        "default_transforms = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
        "apply_transforms(kg, default_transforms)\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
<<<<<<< HEAD
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
=======
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "source": [
        "We can save and load our knowledge graphs as follows."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 9,
=======
      "execution_count": 12,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
<<<<<<< HEAD
              "KnowledgeGraph(nodes: 14, relationships: 70)"
            ]
          },
          "execution_count": 9,
=======
              "KnowledgeGraph(nodes: 14, relationships: 67)"
            ]
          },
          "execution_count": 12,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kg.save(\"ai_across_years_kg.json\")\n",
        "ai_across_years_kg = KnowledgeGraph.load(\"ai_across_years_kg.json\")\n",
        "ai_across_years_kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using our knowledge graph, we can construct a \"test set generator\" - which will allow us to create queries."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 10,
=======
      "execution_count": 13,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=embedding_model, knowledge_graph=ai_across_years_kg)"
      ]
    },
    {
<<<<<<< HEAD
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kat posted something in the Discord Discussions that had the following for Rate Limit issues with TestGenerator:\n",
        "# NOT SURE IF APPLICABLE OR NOT - NOT EXACTLY THE SAME, NEED TO STUDY BEFORE USING - COULD BE A DIFFERENT HW ASSIGNMENT\n",
        "from langchain.chat_models import init_chat_model \n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter \n",
        "rate_limiter = InMemoryRateLimiter( \n",
        "\trequests_per_second=1, # <-- make a request once every 1 seconds!! \n",
        "\tcheck_every_n_seconds=0.1, # Wake up every 100 ms to check whether allowed to make a request, max_bucket_size=10, # Controls the maximum burst size.\n",
        "\t )\n",
        " model = init_chat_model(\"gpt-4o\", temperature=0, rate_limiter=rate_limiter) \n",
        " generator_llm = LangchainLLMWrapper(model) \n",
        " generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings()) \n",
        "\n",
        "from ragas.testset import TestsetGenerator \n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings) \n",
        "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)\n",
        "\n",
        "# From <https://discord.com/channels/1135695983720792216/1327051742638116894/threads/1342710885277237268> "
      ]
    },
    {
=======
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, we'd like to be able to define the kinds of queries we're generating - which is made simple by Ragas having pre-created a number of different \"QuerySynthesizer\"s.\n",
        "\n",
        "Each of these Synthetsizers is going to tackle a separate kind of query which will be generated from a scenario and a persona.\n",
        "\n",
        "In essence, Ragas will use an LLM to generate a persona of someone who would interact with the data - and then use a scenario to construct a question from that data and persona."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 16,
=======
      "execution_count": 14,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset.synthesizers import default_query_distribution, SingleHopSpecificQuerySynthesizer, MultiHopAbstractQuerySynthesizer, MultiHopSpecificQuerySynthesizer\n",
        "\n",
        "query_distribution = [\n",
        "        (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5),\n",
        "        (MultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25),\n",
        "        (MultiHopSpecificQuerySynthesizer(llm=generator_llm), 0.25),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
<<<<<<< HEAD
        "What are the three types of query synthesizers doing? Describe each one in simple terms.\n",
        "(SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5), - Weighting of 50% of the queries to utilize single-hop (one-step retrievals) and is used to help assess  documents given a straightforward, non-multi-step questions, from the source material.\n",
        "(MultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25), - weighted 25% - creates questons that require synthesizing information from multiple parts of the source material and forms more abstract conclusions. \n",
        "(MultiHopSpecificQuerySynthesizer(llm=generator_llm), 0.25), weighted 25% - generates questions that need multiple pieces of specific information to be combined for the answer.  \n",
        "The mix of query types helps test both basic retrieval and more complex."
=======
        "What are the three types of query synthesizers doing? Describe each one in simple terms.\n"
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can use our `TestSetGenerator` to generate our testset!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 17,
=======
      "execution_count": 15,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "5f5fd8d050d743cd972e995a1ce2e118",
=======
              "model_id": "a8dbba3242fd44159094aac9125f5e19",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "554da8025b0c4da2bbfcebadbce5b839",
=======
              "model_id": "f5d9bc23a9f7478290035d6e0b1479a0",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "af773777a765485d98440cd93b734178",
=======
              "model_id": "f640e151006643a0baa19905a71d82e9",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/11 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
<<<<<<< HEAD
              "      <td>Why Apple Intelligence bad but MLX library good?</td>\n",
              "      <td>[Prompt driven app generation is a commodity a...</td>\n",
              "      <td>Apple Intelligence is considered bad, whereas ...</td>\n",
=======
              "      <td>What significant developments in Large Languag...</td>\n",
              "      <td>[Code may be the best application The ethics o...</td>\n",
              "      <td>In 2023, several significant developments in L...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
<<<<<<< HEAD
              "      <td>What is the cost of using Anthropic's Claude 3...</td>\n",
              "      <td>[gets you OpenAI’s most expensive model, o1. G...</td>\n",
              "      <td>Anthropic’s Claude 3 Haiku model costs $0.25 p...</td>\n",
=======
              "      <td>What are some challenges associated with using...</td>\n",
              "      <td>[Based Development As a computer scientist and...</td>\n",
              "      <td>AI research scientists highlight several chall...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
<<<<<<< HEAD
              "      <td>What is the significance of the new WebRTC API...</td>\n",
              "      <td>[your camera feed with the model and talk abou...</td>\n",
              "      <td>The new WebRTC API introduced by OpenAI is sig...</td>\n",
=======
              "      <td>Wht is AI?</td>\n",
              "      <td>[Simon Willison’s Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>AI, in the context of 2023, refers to Large La...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
<<<<<<< HEAD
              "      <td>How does Vercel's approach to prompt protectio...</td>\n",
              "      <td>[“agents” as dependent on AGI itself. A model ...</td>\n",
              "      <td>Vercel's Malte Ubl mentioned that initially, t...</td>\n",
=======
              "      <td>What insights can be drawn from the leaked Goo...</td>\n",
              "      <td>[easy to follow. The rest of the document incl...</td>\n",
              "      <td>The leaked Google document titled 'We Have No ...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
<<<<<<< HEAD
              "      <td>new york flights cost how much?</td>\n",
              "      <td>[day after that. DeepSeek v3 is a huge 685B pa...</td>\n",
              "      <td>The largest Llama 3 model cost about the same ...</td>\n",
=======
              "      <td>Was the best LLM trained in China for less tha...</td>\n",
              "      <td>[Prompt driven app generation is a commodity a...</td>\n",
              "      <td>The context mentions a question about whether ...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
<<<<<<< HEAD
              "      <td>How has the concept of prompt-driven app gener...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>In 2024, prompt-driven app generation has beco...</td>\n",
=======
              "      <td>What are the ethical and legal challenges asso...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The ethical and legal challenges associated wi...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
<<<<<<< HEAD
              "      <td>How has the environmental impact of AI evolved...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The environmental impact of AI has improved du...</td>\n",
=======
              "      <td>How has OpenAI contributed to the development ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>OpenAI has played a significant role in the de...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
<<<<<<< HEAD
              "      <td>How has the concept of prompt-driven app gener...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>In 2024, prompt-driven app generation has beco...</td>\n",
=======
              "      <td>What are the challenges associated with the bl...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The black box nature of Large Language Models ...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
<<<<<<< HEAD
              "      <td>What were the significant developments in larg...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>In 2024, significant developments in large lan...</td>\n",
=======
              "      <td>What advancements in Claude 3 have contributed...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nreasoning patterns. Another common...</td>\n",
              "      <td>The advancements in Claude 3, particularly wit...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
<<<<<<< HEAD
              "      <td>What were the key developments in Large Langua...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>In 2023, Large Language Models (LLMs) were rec...</td>\n",
=======
              "      <td>How have advancements in multi-modal capabilit...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\ngets you OpenAI’s most expensive m...</td>\n",
              "      <td>In 2024, advancements in multi-modal capabilit...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
<<<<<<< HEAD
              "      <td>What advancements in GPT-4 and related technol...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>In 2023, GPT-4 and related technologies saw si...</td>\n",
=======
              "      <td>How has the development of Claude 3 and other ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nreasoning patterns. Another common...</td>\n",
              "      <td>The development of Claude 3 and other multimod...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
<<<<<<< HEAD
              "0    Why Apple Intelligence bad but MLX library good?   \n",
              "1   What is the cost of using Anthropic's Claude 3...   \n",
              "2   What is the significance of the new WebRTC API...   \n",
              "3   How does Vercel's approach to prompt protectio...   \n",
              "4                     new york flights cost how much?   \n",
              "5   How has the concept of prompt-driven app gener...   \n",
              "6   How has the environmental impact of AI evolved...   \n",
              "7   How has the concept of prompt-driven app gener...   \n",
              "8   What were the significant developments in larg...   \n",
              "9   What were the key developments in Large Langua...   \n",
              "10  What advancements in GPT-4 and related technol...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Prompt driven app generation is a commodity a...   \n",
              "1   [gets you OpenAI’s most expensive model, o1. G...   \n",
              "2   [your camera feed with the model and talk abou...   \n",
              "3   [“agents” as dependent on AGI itself. A model ...   \n",
              "4   [day after that. DeepSeek v3 is a huge 685B pa...   \n",
              "5   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "6   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "7   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "8   [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "9   [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "10  [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   Apple Intelligence is considered bad, whereas ...   \n",
              "1   Anthropic’s Claude 3 Haiku model costs $0.25 p...   \n",
              "2   The new WebRTC API introduced by OpenAI is sig...   \n",
              "3   Vercel's Malte Ubl mentioned that initially, t...   \n",
              "4   The largest Llama 3 model cost about the same ...   \n",
              "5   In 2024, prompt-driven app generation has beco...   \n",
              "6   The environmental impact of AI has improved du...   \n",
              "7   In 2024, prompt-driven app generation has beco...   \n",
              "8   In 2024, significant developments in large lan...   \n",
              "9   In 2023, Large Language Models (LLMs) were rec...   \n",
              "10  In 2023, GPT-4 and related technologies saw si...   \n",
=======
              "0   What significant developments in Large Languag...   \n",
              "1   What are some challenges associated with using...   \n",
              "2                                          Wht is AI?   \n",
              "3   What insights can be drawn from the leaked Goo...   \n",
              "4   Was the best LLM trained in China for less tha...   \n",
              "5   What are the ethical and legal challenges asso...   \n",
              "6   How has OpenAI contributed to the development ...   \n",
              "7   What are the challenges associated with the bl...   \n",
              "8   What advancements in Claude 3 have contributed...   \n",
              "9   How have advancements in multi-modal capabilit...   \n",
              "10  How has the development of Claude 3 and other ...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Code may be the best application The ethics o...   \n",
              "1   [Based Development As a computer scientist and...   \n",
              "2   [Simon Willison’s Weblog Subscribe Stuff we fi...   \n",
              "3   [easy to follow. The rest of the document incl...   \n",
              "4   [Prompt driven app generation is a commodity a...   \n",
              "5   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "6   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "7   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "8   [<1-hop>\\n\\nreasoning patterns. Another common...   \n",
              "9   [<1-hop>\\n\\ngets you OpenAI’s most expensive m...   \n",
              "10  [<1-hop>\\n\\nreasoning patterns. Another common...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   In 2023, several significant developments in L...   \n",
              "1   AI research scientists highlight several chall...   \n",
              "2   AI, in the context of 2023, refers to Large La...   \n",
              "3   The leaked Google document titled 'We Have No ...   \n",
              "4   The context mentions a question about whether ...   \n",
              "5   The ethical and legal challenges associated wi...   \n",
              "6   OpenAI has played a significant role in the de...   \n",
              "7   The black box nature of Large Language Models ...   \n",
              "8   The advancements in Claude 3, particularly wit...   \n",
              "9   In 2024, advancements in multi-modal capabilit...   \n",
              "10  The development of Claude 3 and other multimod...   \n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   single_hop_specifc_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  "
            ]
          },
<<<<<<< HEAD
          "execution_count": 17,
=======
          "execution_count": 15,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testset = generator.generate(testset_size=10, query_distribution=query_distribution)\n",
        "testset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Abstracted SDG\n",
        "\n",
        "The above method is the full process - but we can shortcut that using the provided abstractions!\n",
        "\n",
        "This will generate our knowledge graph under the hood, and will - from there - generate our personas and scenarios to construct our queries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 18,
=======
      "execution_count": 21,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "69b405e03b394058924e2925af0c254f",
=======
              "model_id": "064acedb4dfe4569ad1850a00b362e44",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "6ae21f1d69804add9e53f9730fe772ce",
=======
              "model_id": "1e3828f03d6a4e8bb945a4991a028d8a",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "130d4cf799b54bb0969bd36aacdae889",
=======
              "model_id": "df6e0abe83f04241aea36324180a844b",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "cc33f16ca6dd40c19687b8b0e4fb8d5a",
=======
              "model_id": "e532957aaeb54ae7a8f1838fa6f9f281",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "9e1373ff34f246a8a4f1837ad532790f",
=======
              "model_id": "7ac7c0912ea748f5813a13330cd325ea",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/26 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "129aa34f0353465488f5a9fe163f7a87",
=======
              "model_id": "4fae35c0e0134cdbacc6ff946075fa2b",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "0552dfa01e7243b682933f3842e9c142",
=======
              "model_id": "d6d3708e5b0d444b81cf4ba82e19beaa",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "1ca4d98b3db84b679122760b5e956f95",
=======
              "model_id": "02f438de9f7d49259dfafeb7b12c048b",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "6946390d58c64957a406a50a6b76d092",
=======
              "model_id": "6b4d6adab72140c99f276f3bc0bbb88b",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 19,
=======
      "execution_count": 22,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
<<<<<<< HEAD
              "      <td>What advancements have been made with Claude 3...</td>\n",
              "      <td>[Prompt driven app generation is a commodity a...</td>\n",
              "      <td>Anthropic's Claude 3 series launched in March,...</td>\n",
=======
              "      <td>Wut has Meta dun in the feeld of LLMs this yeer?</td>\n",
              "      <td>[Code may be the best application The ethics o...</td>\n",
              "      <td>In February, Meta released Llama, and in July,...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
<<<<<<< HEAD
              "      <td>Wht are the key features and pricing details o...</td>\n",
              "      <td>[gets you OpenAI’s most expensive model, o1. G...</td>\n",
              "      <td>Anthropic’s Claude 3 Haiku, released in March,...</td>\n",
=======
              "      <td>What significant event related to AI ethics oc...</td>\n",
              "      <td>[Based Development As a computer scientist and...</td>\n",
              "      <td>In September last year, the term 'prompt injec...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
<<<<<<< HEAD
              "      <td>What are the key features and impacts of Claud...</td>\n",
              "      <td>[your camera feed with the model and talk abou...</td>\n",
              "      <td>Claude 3.5 Sonnet introduced groundbreaking fe...</td>\n",
=======
              "      <td>Whaat are the key highlights from Simon Willis...</td>\n",
              "      <td>[Simon Willison’s Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>Simon Willison's Weblog highlights that 2023 w...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
<<<<<<< HEAD
              "      <td>How does Apple's MLX library enhance the perfo...</td>\n",
              "      <td>[“agents” as dependent on AGI itself. A model ...</td>\n",
              "      <td>Apple's MLX library, described as an array fra...</td>\n",
=======
              "      <td>What role does Stanford Alpaca play in the dev...</td>\n",
              "      <td>[easy to follow. The rest of the document incl...</td>\n",
              "      <td>Stanford Alpaca is associated with the acceler...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
<<<<<<< HEAD
              "      <td>How have advancements in GPT-4 and the competi...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>Advancements in GPT-4 and the subsequent compe...</td>\n",
=======
              "      <td>What are the ethical concerns associated with ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The ethical concerns associated with the gulli...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
<<<<<<< HEAD
              "      <td>How has the evolution of prompt-driven app gen...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>In 2024, prompt-driven app generation has beco...</td>\n",
=======
              "      <td>Why are Large Language Models (LLMs) considere...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>Large Language Models (LLMs) are considered bl...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
<<<<<<< HEAD
              "      <td>How have advancements in GPT-4 and the competi...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>Advancements in GPT-4 and the subsequent compe...</td>\n",
=======
              "      <td>What are the ethical concerns related to the g...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The ethical concerns related to the gullibilit...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
<<<<<<< HEAD
              "      <td>How did the shift from universal access to sub...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The shift from universal access to subscriptio...</td>\n",
=======
              "      <td>How do the ethics of AI and the gullibility of...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The ethics of AI and the gullibility of langua...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
<<<<<<< HEAD
              "      <td>How has Google's Gemini series contributed to ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>Google's Gemini series has significantly contr...</td>\n",
=======
              "      <td>How has the development of models surpassing G...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>In 2024, the development of models surpassing ...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
<<<<<<< HEAD
              "      <td>How have the advancements in GPT-4o and its pr...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>The advancements in GPT-4o, particularly its p...</td>\n",
=======
              "      <td>How does the training efficiency of DeepSeek v...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nday after that. DeepSeek v3 is a h...</td>\n",
              "      <td>DeepSeek v3, a 685B parameter model, is one of...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
<<<<<<< HEAD
              "      <td>How did the developments in Large Language Mod...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>In 2023, Large Language Models (LLMs) marked a...</td>\n",
=======
              "      <td>How did the breaking of the GPT-4 barrier in 2...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>In 2024, the breaking of the GPT-4 barrier sig...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
<<<<<<< HEAD
              "      <td>How did the developments in GPT-4 and Claude A...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>In 2023, GPT-4 and Claude Artifacts significan...</td>\n",
=======
              "      <td>How has the introduction of GPT-4o impacted th...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>The introduction of GPT-4o has significantly i...</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
<<<<<<< HEAD
              "0   What advancements have been made with Claude 3...   \n",
              "1   Wht are the key features and pricing details o...   \n",
              "2   What are the key features and impacts of Claud...   \n",
              "3   How does Apple's MLX library enhance the perfo...   \n",
              "4   How have advancements in GPT-4 and the competi...   \n",
              "5   How has the evolution of prompt-driven app gen...   \n",
              "6   How have advancements in GPT-4 and the competi...   \n",
              "7   How did the shift from universal access to sub...   \n",
              "8   How has Google's Gemini series contributed to ...   \n",
              "9   How have the advancements in GPT-4o and its pr...   \n",
              "10  How did the developments in Large Language Mod...   \n",
              "11  How did the developments in GPT-4 and Claude A...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Prompt driven app generation is a commodity a...   \n",
              "1   [gets you OpenAI’s most expensive model, o1. G...   \n",
              "2   [your camera feed with the model and talk abou...   \n",
              "3   [“agents” as dependent on AGI itself. A model ...   \n",
              "4   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "5   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "6   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "7   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "8   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "9   [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
=======
              "0    Wut has Meta dun in the feeld of LLMs this yeer?   \n",
              "1   What significant event related to AI ethics oc...   \n",
              "2   Whaat are the key highlights from Simon Willis...   \n",
              "3   What role does Stanford Alpaca play in the dev...   \n",
              "4   What are the ethical concerns associated with ...   \n",
              "5   Why are Large Language Models (LLMs) considere...   \n",
              "6   What are the ethical concerns related to the g...   \n",
              "7   How do the ethics of AI and the gullibility of...   \n",
              "8   How has the development of models surpassing G...   \n",
              "9   How does the training efficiency of DeepSeek v...   \n",
              "10  How did the breaking of the GPT-4 barrier in 2...   \n",
              "11  How has the introduction of GPT-4o impacted th...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Code may be the best application The ethics o...   \n",
              "1   [Based Development As a computer scientist and...   \n",
              "2   [Simon Willison’s Weblog Subscribe Stuff we fi...   \n",
              "3   [easy to follow. The rest of the document incl...   \n",
              "4   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "5   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "6   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "7   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "8   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "9   [<1-hop>\\n\\nday after that. DeepSeek v3 is a h...   \n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "10  [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "11  [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "\n",
              "                                            reference  \\\n",
<<<<<<< HEAD
              "0   Anthropic's Claude 3 series launched in March,...   \n",
              "1   Anthropic’s Claude 3 Haiku, released in March,...   \n",
              "2   Claude 3.5 Sonnet introduced groundbreaking fe...   \n",
              "3   Apple's MLX library, described as an array fra...   \n",
              "4   Advancements in GPT-4 and the subsequent compe...   \n",
              "5   In 2024, prompt-driven app generation has beco...   \n",
              "6   Advancements in GPT-4 and the subsequent compe...   \n",
              "7   The shift from universal access to subscriptio...   \n",
              "8   Google's Gemini series has significantly contr...   \n",
              "9   The advancements in GPT-4o, particularly its p...   \n",
              "10  In 2023, Large Language Models (LLMs) marked a...   \n",
              "11  In 2023, GPT-4 and Claude Artifacts significan...   \n",
=======
              "0   In February, Meta released Llama, and in July,...   \n",
              "1   In September last year, the term 'prompt injec...   \n",
              "2   Simon Willison's Weblog highlights that 2023 w...   \n",
              "3   Stanford Alpaca is associated with the acceler...   \n",
              "4   The ethical concerns associated with the gulli...   \n",
              "5   Large Language Models (LLMs) are considered bl...   \n",
              "6   The ethical concerns related to the gullibilit...   \n",
              "7   The ethics of AI and the gullibility of langua...   \n",
              "8   In 2024, the development of models surpassing ...   \n",
              "9   DeepSeek v3, a 685B parameter model, is one of...   \n",
              "10  In 2024, the breaking of the GPT-4 barrier sig...   \n",
              "11  The introduction of GPT-4o has significantly i...   \n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
<<<<<<< HEAD
          "execution_count": 19,
=======
          "execution_count": 22,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vSRr2MXk0P_"
      },
      "source": [
        "We'll need to provide our LangSmith API key, and set tracing to \"true\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLDUsLJg43k7"
      },
      "source": [
        "# 🤝 BREAKOUT ROOM #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SLtk1GtnyoY"
      },
      "source": [
        "## Task 4: LangSmith Dataset\n",
        "\n",
        "Now we can move on to creating a dataset for LangSmith!\n",
        "\n",
        "First, we'll need to create a dataset on LangSmith using the `Client`!\n",
        "\n",
        "We'll name our Dataset to make it easy to work with later."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 20,
=======
      "execution_count": 24,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "TLgm6OjvYSsm"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"State of AI Across the Years!\"\n",
        "\n",
        "langsmith_dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"State of AI Across the Years!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64SmXMBnzXWm"
      },
      "source": [
        "We'll iterate through the RAGAS created dataframe - and add each example to our created dataset!\n",
        "\n",
        "> NOTE: We need to conform the outputs to the expected format - which in this case is: `question` and `answer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 21,
=======
      "execution_count": null,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "8nFQ6di_XnY7"
      },
      "outputs": [],
      "source": [
        "for data_row in dataset.to_pandas().iterrows():\n",
        "  client.create_example(\n",
        "      inputs={\n",
        "          \"question\": data_row[1][\"user_input\"]\n",
        "      },\n",
        "      outputs={\n",
        "          \"answer\": data_row[1][\"reference\"]\n",
        "      },\n",
        "      metadata={\n",
        "          \"context\": data_row[1][\"reference_contexts\"]\n",
        "      },\n",
        "      dataset_id=langsmith_dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6EbQVyZq-2j"
      },
      "source": [
        "## Basic RAG Chain\n",
        "\n",
        "Time for some RAG!\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 22,
=======
      "execution_count": 34,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "4njbUAIsaYjB"
      },
      "outputs": [],
      "source": [
        "rag_documents = docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQorBy8H1AZR"
      },
      "source": [
        "To keep things simple, we'll just use LangChain's recursive character text splitter!\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 23,
=======
      "execution_count": 35,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "qWo3Ajaragv1"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "rag_documents = text_splitter.split_documents(rag_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kghuTb9R01oO"
      },
      "source": [
        "We'll create our vectorstore using OpenAI's [`text-embedding-3-small`](https://platform.openai.com/docs/guides/embeddings/embedding-models) embedding model."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 24,
=======
      "execution_count": 36,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "UwfJCzP3aqKI"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpCLS-a01Ft2"
      },
      "source": [
        "As usual, we will power our RAG application with Qdrant!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 25,
=======
      "execution_count": 37,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "58Ypj_NgbEsi"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents=rag_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"State of AI\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 26,
=======
      "execution_count": 53,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "SbKSjfSkbTYo"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxUOMaQX1K2N"
      },
      "source": [
        "To get the \"A\" in RAG, we'll provide a prompt."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 27,
=======
      "execution_count": 54,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "1sLeY1oWbVqO"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZnHDh4e1Ou5"
      },
      "source": [
        "For our LLM, we will be using TogetherAI's endpoints as well!\n",
        "\n",
        "We're going to be using Meta Llama 3.1 70B Instruct Turbo - a powerful model which should get us powerful results!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 28,
=======
      "execution_count": 55,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "6nx-ue1XbciV"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmTL6-pc1ZGz"
      },
      "source": [
        "Finally, we can set-up our RAG LCEL chain!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 29,
=======
      "execution_count": 56,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "TjWj0OLIbbFc"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | rag_prompt | llm | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 30,
=======
      "execution_count": 57,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WQ7bEweo4IIb",
        "outputId": "d161b269-f799-4920-d6ce-c202f6e783aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
<<<<<<< HEAD
              "'Agents refer to AI systems that can act on behalf of users, but the term is considered vague and lacks a clear, widely understood definition. There are different interpretations of what agents are, including those that perform tasks like a travel agent and those that utilize large language models (LLMs) with access to tools for problem-solving. Overall, there is skepticism about their utility, largely due to challenges in determining truth from fiction, which is critical for effective decision-making.'"
            ]
          },
          "execution_count": 30,
=======
              "'Agents are infuriatingly vague AI systems that are often thought of as capable of acting on your behalf, similar to a travel agent. There are differing interpretations of what \"agents\" are, with some viewing them as AI that can run tools in a loop to solve problems. However, the term lacks a clear and widely understood definition, and there are few examples of such systems running in production despite prototypes. The concept of agents is still seen as perpetually \"coming soon.\"'"
            ]
          },
          "execution_count": 57,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke({\"question\" : \"What are Agents?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9hBh5YPrdGJ"
      },
      "source": [
        "## LangSmith Evaluation Set-up\n",
        "\n",
        "We'll use OpenAI's GPT-4o as our evaluation LLM for our base Evaluators."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 31,
=======
      "execution_count": 58,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "gfwPYdIkcvpF"
      },
      "outputs": [],
      "source": [
        "eval_llm = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b8pToKH2K28"
      },
      "source": [
        "We'll be using a number of evaluators - from LangSmith provided evaluators, to a few custom evaluators!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 32,
=======
      "execution_count": 59,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "PXSG-_ajckp6"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})\n",
        "\n",
        "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"helpfulness\": (\n",
        "                \"Is this submission helpful to the user,\"\n",
        "                \" taking into account the correct reference answer?\"\n",
        "            )\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": run.outputs[\"output\"],\n",
        "        \"reference\": example.outputs[\"answer\"],\n",
        "        \"input\": example.inputs[\"question\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "dope_or_nope_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"dopeness\": \"Is this submission dope, lit, or cool?\",\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0SQP_FoCetP"
      },
      "source": [
        "#### 🏗️ Activity #2:\n",
        "\n",
        "Highlight what each evaluator is evaluating.\n",
        "\n",
        "- `qa_evaluator`:\n",
        "- `labeled_helpfulness_evaluator`:\n",
        "- `dope_or_nope_evaluator`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R35sQMHVrnpl"
      },
      "source": [
        "## LangSmith Evaluation"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 34,
=======
      "execution_count": 60,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "122b1bd1f0e9417a8dcb57d4eebe4d2e",
            "e0c233ad01604540a6c873f4a731982d",
            "e9a01115c75b499884f7e0ef32e9e599",
            "5faba4ad609448b2b49024add4ad3b8e",
            "ef25efa751304e4699910f1fbc14345f",
            "0b44cb0f8e34446c8dde668a75d3d8ad",
            "edaac6587b2d4bd5be52b89bb097f99f",
            "7cb241365f604419af454c1c28de197a",
            "9cf586576ff44dba86ba2eb389593c61",
            "849b5c95008541d49f1ceedf0a59ac60",
            "f3665a86662746c4ac7cb0796604781d"
          ]
        },
        "id": "t7t_Uz0tdumL",
        "outputId": "d684e218-294e-4dc3-c8de-a01d397f021c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "View the evaluation results for experiment: 'frosty-availability-23' at:\n",
            "https://smith.langchain.com/o/2928bae9-0c08-4543-b4d4-5b2f0a267e9d/datasets/00704a1a-6702-4a89-8b26-920db81b70ae/compare?selectedSessions=f1e25c62-0089-4995-88f3-d697d73f79f9\n",
=======
            "View the evaluation results for experiment: 'helpful-level-61' at:\n",
            "https://smith.langchain.com/o/117cfda3-8a09-4ba4-9922-07b45fd73803/datasets/25fa804e-0ce3-4848-9cd1-c83d91988e78/compare?selectedSessions=a7b69e26-ad7f-4859-856e-399ef1c0f360\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "97427d2e8e204cb689dbca84839f3261",
=======
              "model_id": "168eaac5fd1b4d89abe3ac11c48f27f3",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.output</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.correctness</th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.dopeness</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
<<<<<<< HEAD
              "      <td>How did the developments in GPT-4 and Claude A...</td>\n",
              "      <td>The developments in GPT-4 and Claude 3.5 Sonne...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2023, GPT-4 and Claude Artifacts significan...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.506365</td>\n",
              "      <td>c59f1bae-1f08-4850-a9ef-0c70a42536f4</td>\n",
              "      <td>8c56a0c7-8514-4b54-a6cd-2b465fccce2d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How did the developments in Large Language Mod...</td>\n",
              "      <td>In 2023, the developments in Large Language Mo...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2023, Large Language Models (LLMs) marked a...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.223102</td>\n",
              "      <td>e2093edc-a5c9-4077-ae52-81962af78e16</td>\n",
              "      <td>fe03f3b9-2404-4348-8fb1-5d04dad9bfcb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How have the advancements in GPT-4o and its pr...</td>\n",
              "      <td>The advancements in GPT-4o, which is significa...</td>\n",
              "      <td>None</td>\n",
              "      <td>The advancements in GPT-4o, particularly its p...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.262459</td>\n",
              "      <td>43ad4982-4ca3-4522-aa0d-c3e82b7aaa32</td>\n",
              "      <td>2d7fad8c-d4ed-443b-9f8c-764a5d6ce2f1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How has Google's Gemini series contributed to ...</td>\n",
              "      <td>Google's Gemini series has significantly contr...</td>\n",
              "      <td>None</td>\n",
              "      <td>Google's Gemini series has significantly contr...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.511032</td>\n",
              "      <td>a6ff2de5-84cb-4426-a68e-36d5303dac3f</td>\n",
              "      <td>6cd8e151-2fd5-41f3-b0b2-0738c4787fe3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How did the shift from universal access to sub...</td>\n",
              "      <td>The shift from universal access to subscriptio...</td>\n",
              "      <td>None</td>\n",
              "      <td>The shift from universal access to subscriptio...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.268907</td>\n",
              "      <td>3e58700d-9104-47a9-a42f-167296fc4304</td>\n",
              "      <td>fcb44388-0720-423a-b327-16770993fe5f</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How have advancements in GPT-4 and the competi...</td>\n",
              "      <td>Advancements in GPT-4 and the competition from...</td>\n",
              "      <td>None</td>\n",
              "      <td>Advancements in GPT-4 and the subsequent compe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.218723</td>\n",
              "      <td>69ee214c-8534-464b-b645-45fab132366b</td>\n",
              "      <td>d0f02ee4-9702-490e-a08a-45a10614d192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How has the evolution of prompt-driven app gen...</td>\n",
              "      <td>The evolution of prompt-driven app generation ...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2024, prompt-driven app generation has beco...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.321675</td>\n",
              "      <td>614d1d8d-c14c-420b-9263-61a8a13babb2</td>\n",
              "      <td>504accdb-37dc-4530-8cbd-00eb00b0a4fb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How have advancements in GPT-4 and the competi...</td>\n",
              "      <td>Advancements in GPT-4 and the competition from...</td>\n",
              "      <td>None</td>\n",
              "      <td>Advancements in GPT-4 and the subsequent compe...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.199813</td>\n",
              "      <td>14ee9a64-f2ce-4371-ac08-0e9598e06475</td>\n",
              "      <td>9fa8e7da-14db-4d07-aec1-ecf7cfe2b8f1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does Apple's MLX library enhance the perfo...</td>\n",
              "      <td>Apple's MLX library enhances the performance o...</td>\n",
              "      <td>None</td>\n",
              "      <td>Apple's MLX library, described as an array fra...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.162245</td>\n",
              "      <td>19cd73dd-141c-4815-b890-0f47bfdd303a</td>\n",
              "      <td>ee91d9b9-0b88-4fc8-9c57-96a10d6c2934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What are the key features and impacts of Claud...</td>\n",
              "      <td>Claude 3.5 Sonnet introduced a significant fea...</td>\n",
              "      <td>None</td>\n",
              "      <td>Claude 3.5 Sonnet introduced groundbreaking fe...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.180664</td>\n",
              "      <td>eaea04ea-774a-4ef1-a08f-6d87be1db48d</td>\n",
              "      <td>60109b04-1e39-4f9c-8c6d-5533c5c09a1a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Wht are the key features and pricing details o...</td>\n",
              "      <td>Claude 3 series launched by Anthropic includes...</td>\n",
              "      <td>None</td>\n",
              "      <td>Anthropic’s Claude 3 Haiku, released in March,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.224178</td>\n",
              "      <td>85d7e0e5-bd4d-40c7-8885-ceef7b7dd39d</td>\n",
              "      <td>302ca251-d04b-4542-bba7-4708d8069626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What advancements have been made with Claude 3...</td>\n",
              "      <td>Claude 3 has seen several advancements, partic...</td>\n",
              "      <td>None</td>\n",
              "      <td>Anthropic's Claude 3 series launched in March,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.154037</td>\n",
              "      <td>6e150429-bb2b-4696-a86c-b593dc842ed9</td>\n",
              "      <td>ea12726d-0e36-416b-9835-37058425c803</td>\n",
=======
              "      <td>How has the introduction of GPT-4o impacted th...</td>\n",
              "      <td>The introduction of GPT-4o has significantly i...</td>\n",
              "      <td>None</td>\n",
              "      <td>The introduction of GPT-4o has significantly i...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.310405</td>\n",
              "      <td>e749dc1c-f46a-4e61-9742-daa3d275671c</td>\n",
              "      <td>aad0afa8-5daa-4b15-8c59-e96d575b6f2c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How did the breaking of the GPT-4 barrier in 2...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2024, the breaking of the GPT-4 barrier sig...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.700944</td>\n",
              "      <td>e0f5c6c4-fb8e-411e-8399-95212616c92b</td>\n",
              "      <td>e113b68f-0aae-4f59-91c0-8d70158adee2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the training efficiency of DeepSeek v...</td>\n",
              "      <td>DeepSeek v3 was trained on 2,788,000 H800 GPU ...</td>\n",
              "      <td>None</td>\n",
              "      <td>DeepSeek v3, a 685B parameter model, is one of...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.872102</td>\n",
              "      <td>28816b79-8c83-49b7-90f3-dc5a99ee6a7f</td>\n",
              "      <td>99ad5aba-b54f-4c82-ac90-3a167e83bc7d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How has the development of models surpassing G...</td>\n",
              "      <td>The development of models surpassing GPT-4 has...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2024, the development of models surpassing ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.169376</td>\n",
              "      <td>757f3ec9-c32f-4d24-a2f7-d02b88fdb499</td>\n",
              "      <td>c761fa2e-5329-4b05-bf53-0b61e72ff75d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do the ethics of AI and the gullibility of...</td>\n",
              "      <td>The ethics of AI and the gullibility of langua...</td>\n",
              "      <td>None</td>\n",
              "      <td>The ethics of AI and the gullibility of langua...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.404639</td>\n",
              "      <td>bf0ac2b1-9738-4e30-8f8b-1b5d9e8c3b04</td>\n",
              "      <td>65a8f286-b6c4-48eb-bba4-7ed8a55d314e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What are the ethical concerns related to the g...</td>\n",
              "      <td>The ethical concerns related to the gullibilit...</td>\n",
              "      <td>None</td>\n",
              "      <td>The ethical concerns related to the gullibilit...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.918669</td>\n",
              "      <td>d312a7f8-fd86-4df7-9d4d-6f9387a11412</td>\n",
              "      <td>ff4201c9-470d-4d6f-9c2b-40c64b053570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Why are Large Language Models (LLMs) considere...</td>\n",
              "      <td>Large Language Models (LLMs) are considered bl...</td>\n",
              "      <td>None</td>\n",
              "      <td>Large Language Models (LLMs) are considered bl...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.259788</td>\n",
              "      <td>28659332-3637-4cee-9018-98d587575f5b</td>\n",
              "      <td>d18b29b1-6422-4b0f-b761-d96a004c0243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are the ethical concerns associated with ...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>The ethical concerns associated with the gulli...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.766063</td>\n",
              "      <td>2da5c189-eb9e-419b-9c93-e3247d9f154c</td>\n",
              "      <td>409b4ec8-294d-4d8a-ae1e-e33c5de68752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What role does Stanford Alpaca play in the dev...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>Stanford Alpaca is associated with the acceler...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.933643</td>\n",
              "      <td>1c5369b1-7b7e-476d-9d28-816ce1a360f6</td>\n",
              "      <td>9f92ee8e-88f0-4525-bf82-d2b42d4cbefb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Whaat are the key highlights from Simon Willis...</td>\n",
              "      <td>The key highlights from Simon Willison's Weblo...</td>\n",
              "      <td>None</td>\n",
              "      <td>Simon Willison's Weblog highlights that 2023 w...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.307175</td>\n",
              "      <td>012f7bdc-73f0-4ad8-8a30-e89edbb0a0e7</td>\n",
              "      <td>90b1d8bb-f737-4a95-95db-a727cd1cc64a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What significant event related to AI ethics oc...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>In September last year, the term 'prompt injec...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.019731</td>\n",
              "      <td>5719d5a1-4355-41b4-aabc-dd1c10357462</td>\n",
              "      <td>fa53ccd9-4ca8-4a93-90d8-f233ec7c9536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Wut has Meta dun in the feeld of LLMs this yeer?</td>\n",
              "      <td>Meta has produced better-than-GPT-3 class mode...</td>\n",
              "      <td>None</td>\n",
              "      <td>In February, Meta released Llama, and in July,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.670921</td>\n",
              "      <td>dcc029aa-596c-44f0-89c4-c54fa92184ea</td>\n",
              "      <td>35501cf6-6262-4a8d-a73d-23aac7a30ca5</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
<<<<<<< HEAD
              "<ExperimentResults frosty-availability-23>"
            ]
          },
          "execution_count": 34,
=======
              "<ExperimentResults helpful-level-61>"
            ]
          },
          "execution_count": 60,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        dope_or_nope_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
<<<<<<< HEAD
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
=======
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "Nq7fCVinrpI4"
      },
      "source": [
        "## Dope-ifying Our Application\n",
        "\n",
        "We'll be making a few changes to our RAG chain to increase its performance on our SDG evaluation test dataset!\n",
        "\n",
        "- Include a \"dope\" prompt augmentation\n",
        "- Use larger chunks\n",
        "- Improve the retriever model to: `text-embedding-3-large`\n",
        "\n",
        "Let's see how this changes our evaluation!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 35,
=======
      "execution_count": 61,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "z56pXwyUgFUt"
      },
      "outputs": [],
      "source": [
        "DOPE_RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
        "\n",
        "You must answer the questions in a dope way, be cool!\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "dope_rag_prompt = ChatPromptTemplate.from_template(DOPE_RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 36,
=======
      "execution_count": 62,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "rZLcTstJgfv5"
      },
      "outputs": [],
      "source": [
        "rag_documents = docs"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 37,
=======
      "execution_count": 63,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "-LYsyirngj6n"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "rag_documents = text_splitter.split_documents(rag_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spldiPuTCzDO"
      },
      "source": [
        "#### ❓Question #2:\n",
        "\n",
<<<<<<< HEAD
        "Why would modifying our chunk size modify the performance of our application?\n",
        "Larger chunkc sizes provide more context (more complete) within a segment but the larger size means fewer chunks can fit in the LLM's context window.  The larger chunk sizes might provide better context with the surrounding information but that can also mean it includes irrelevant information.  The larger chuncks also create fewer total vectors to search through but the vectors have greater context but you'll lose out on the granularity of matching specific details.  The larger chunk size will keep related concepts together and provide better context for the LLMs to generate answers and it needs to be manged to fit multiple chunks in context."
=======
        "Why would modifying our chunk size modify the performance of our application?"
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 38,
=======
      "execution_count": 64,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "b9MI2Bm2go1r"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBbjG6cKC8BQ"
      },
      "source": [
        "#### ❓Question #3:\n",
        "\n",
<<<<<<< HEAD
        "Why would modifying our embedding model modify the performance of our application?\n",
        " Using a larger model can better distinguish between subtle differences in meaning due to the larger dimensionality.  The larger model should be more accurate as it has been trained on more data and should pick up on more complex linguistic patterns.  The better embeddings lead to more accurate similarity searches in the vector databas and reduce hallucinations."
=======
        "Why would modifying our embedding model modify the performance of our application?"
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 39,
=======
      "execution_count": 65,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "hVUY25FKgxXx"
      },
      "outputs": [],
      "source": [
        "vectorstore = Qdrant.from_documents(\n",
        "    documents=rag_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"AI Across Years (Augmented)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 40,
=======
      "execution_count": 66,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "Q4TOZNYIg2v1"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqYGFrnKDB91"
      },
      "source": [
        "Setting up our new and improved DOPE RAG CHAIN."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 41,
=======
      "execution_count": 67,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "id": "HqnTqeXMhAdx"
      },
      "outputs": [],
      "source": [
        "dope_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | dope_rag_prompt | llm | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21pTxoqJDI1Y"
      },
      "source": [
        "Let's test it on the same output that we saw before."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 42,
=======
      "execution_count": 68,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OfZZ3MoN3fKv",
        "outputId": "d65722dd-92c2-4e4e-9cca-c42ee6f3f208"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
<<<<<<< HEAD
              "'Agents, my friend, are these mysterious AI systems that are supposed to roll up their sleeves and act on your behalf, kind of like a digital assistant or a travel agent. But here\\'s the kicker—they\\'re frustratingly vague and not clearly defined, leading to a whole lot of confusion. Some folks think of them as LLMs (Large Language Models) with tools, while others have a more traditional view of agents as things that make choices for you. The buzz around them is real, but actual examples in production? That\\'s like chasing a phantom—still waiting on that! So, in a nutshell, agents are those elusive AI helpers we\\'re all curious about, but they’re still in the \"coming soon\" phase, plagued by challenges like gullibility and a lack of clarity.'"
            ]
          },
          "execution_count": 42,
=======
              "'Agents? Man, they’re this super vague concept in the AI world. Some folks think of them as digital assistants that act on your behalf—like a travel agent. Others see them as AI models with tools, looping through tasks to solve problems. But here’s the kicker: the term is so fuzzy that it leaves you scratching your head, since everyone seems to have their own take on what it really means. Plus, there’s this whole issue of gullibility—how can these agents make smart choices if they can’t tell reality from fiction? So yeah, they’re like this elusive dream still waiting for a real breakthrough. 💫'"
            ]
          },
          "execution_count": 68,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dope_rag_chain.invoke({\"question\" : \"what are Agents?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpj7v1inDLnQ"
      },
      "source": [
        "Finally, we can evaluate the new chain on the same test set!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 43,
=======
      "execution_count": 69,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "bf8dcc0895054529af356da401c513f6",
            "7dce19ac55264f2b88a0e4730e55867b",
            "2a0755d4476543feb4a64538e3e37213",
            "158212a630f04cbd884c937f2f60f5c8",
            "11c7f66acc1d45be9517d0addf49331e",
            "ddffd834e09940a4bd3874c3f39b4e21",
            "ef63c3b2d51e452da03cdae5d9b034be",
            "c20b539cd70b4ba99601ad1d69fd9cec",
            "a6d681eeafa44d18b933a4c5dec88382",
            "d1d54ccd56494c4d831f71b416a1f880",
            "530f696feefe499da08c6312047379b2"
          ]
        },
        "id": "Dx11S2b-hIM8",
        "outputId": "d3a3ea78-aa32-4bd2-8c2a-d0d0303695c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "View the evaluation results for experiment: 'perfect-click-58' at:\n",
            "https://smith.langchain.com/o/2928bae9-0c08-4543-b4d4-5b2f0a267e9d/datasets/00704a1a-6702-4a89-8b26-920db81b70ae/compare?selectedSessions=cba5fe52-2f38-42e5-903b-b3198157a281\n",
=======
            "View the evaluation results for experiment: 'artistic-airplane-98' at:\n",
            "https://smith.langchain.com/o/117cfda3-8a09-4ba4-9922-07b45fd73803/datasets/25fa804e-0ce3-4848-9cd1-c83d91988e78/compare?selectedSessions=42ea0e06-22a8-4bfb-a5bc-760a0cf4d280\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "7da7a7adab234d848344be1f5ac5c387",
=======
              "model_id": "f72d571a85c4495db152af05ef004ffd",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.output</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.correctness</th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.dopeness</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
<<<<<<< HEAD
              "      <td>How did the developments in GPT-4 and Claude A...</td>\n",
              "      <td>Yo, let me break it down for you! The developm...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2023, GPT-4 and Claude Artifacts significan...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.127867</td>\n",
              "      <td>c59f1bae-1f08-4850-a9ef-0c70a42536f4</td>\n",
              "      <td>abed975e-8b48-46b8-ac25-34a55c03bbc8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How did the developments in Large Language Mod...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2023, Large Language Models (LLMs) marked a...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.983946</td>\n",
              "      <td>e2093edc-a5c9-4077-ae52-81962af78e16</td>\n",
              "      <td>2996d568-66f5-4f25-af14-03666cbfd7bc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How have the advancements in GPT-4o and its pr...</td>\n",
              "      <td>Yo, check it out! With the rollout of GPT-4o, ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The advancements in GPT-4o, particularly its p...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.887934</td>\n",
              "      <td>43ad4982-4ca3-4522-aa0d-c3e82b7aaa32</td>\n",
              "      <td>7ba8d7bc-81ae-48d3-9228-ad2a106150dd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How has Google's Gemini series contributed to ...</td>\n",
              "      <td>Yo, Google's Gemini series has totally flipped...</td>\n",
              "      <td>None</td>\n",
              "      <td>Google's Gemini series has significantly contr...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3.330325</td>\n",
              "      <td>a6ff2de5-84cb-4426-a68e-36d5303dac3f</td>\n",
              "      <td>e5711b98-0b86-47d1-96e1-83623b61f175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How did the shift from universal access to sub...</td>\n",
              "      <td>The shift from universal access to subscriptio...</td>\n",
              "      <td>None</td>\n",
              "      <td>The shift from universal access to subscriptio...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.200185</td>\n",
              "      <td>3e58700d-9104-47a9-a42f-167296fc4304</td>\n",
              "      <td>3a18818b-7c2b-417e-a9c1-712527e6b223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How have advancements in GPT-4 and the competi...</td>\n",
              "      <td>Yo, the landscape of large language models (LL...</td>\n",
              "      <td>None</td>\n",
              "      <td>Advancements in GPT-4 and the subsequent compe...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.781439</td>\n",
              "      <td>69ee214c-8534-464b-b645-45fab132366b</td>\n",
              "      <td>e88c0b79-de2f-47e8-8a42-deaff65ce4f0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How has the evolution of prompt-driven app gen...</td>\n",
              "      <td>Yo, the evolution of prompt-driven app generat...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2024, prompt-driven app generation has beco...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.236311</td>\n",
              "      <td>614d1d8d-c14c-420b-9263-61a8a13babb2</td>\n",
              "      <td>c4409d16-582b-40eb-a9cb-b61e9d6eeb3c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How have advancements in GPT-4 and the competi...</td>\n",
              "      <td>Yo, the landscape of large language models (LL...</td>\n",
              "      <td>None</td>\n",
              "      <td>Advancements in GPT-4 and the subsequent compe...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.525805</td>\n",
              "      <td>14ee9a64-f2ce-4371-ac08-0e9598e06475</td>\n",
              "      <td>f1d90f34-d734-409b-9fbe-d946a9ca67bc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does Apple's MLX library enhance the perfo...</td>\n",
              "      <td>Apple's MLX library is a game changer for runn...</td>\n",
              "      <td>None</td>\n",
              "      <td>Apple's MLX library, described as an array fra...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3.330101</td>\n",
              "      <td>19cd73dd-141c-4815-b890-0f47bfdd303a</td>\n",
              "      <td>a8c99225-c075-457d-8d7f-0b950dde4a3d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What are the key features and impacts of Claud...</td>\n",
              "      <td>Yo, Claude 3.5 Sonnet is a total game changer ...</td>\n",
              "      <td>None</td>\n",
              "      <td>Claude 3.5 Sonnet introduced groundbreaking fe...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.897723</td>\n",
              "      <td>eaea04ea-774a-4ef1-a08f-6d87be1db48d</td>\n",
              "      <td>6d724526-4097-4457-9f3f-f2aed9e77bfa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Wht are the key features and pricing details o...</td>\n",
              "      <td>Yo, check it! Claude 3 comes out swinging with...</td>\n",
              "      <td>None</td>\n",
              "      <td>Anthropic’s Claude 3 Haiku, released in March,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.855615</td>\n",
              "      <td>85d7e0e5-bd4d-40c7-8885-ceef7b7dd39d</td>\n",
              "      <td>c183e90d-430b-4bba-8d4f-b7c205e0724b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What advancements have been made with Claude 3...</td>\n",
              "      <td>Yo, check it out! Claude 3 has stepped up its ...</td>\n",
              "      <td>None</td>\n",
              "      <td>Anthropic's Claude 3 series launched in March,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3.708813</td>\n",
              "      <td>6e150429-bb2b-4696-a86c-b593dc842ed9</td>\n",
              "      <td>55c5b0af-c56e-465e-bb71-375420827ae8</td>\n",
=======
              "      <td>How has the introduction of GPT-4o impacted th...</td>\n",
              "      <td>Yo, the rollout of GPT-4o is such a game chang...</td>\n",
              "      <td>None</td>\n",
              "      <td>The introduction of GPT-4o has significantly i...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.590299</td>\n",
              "      <td>e749dc1c-f46a-4e61-9742-daa3d275671c</td>\n",
              "      <td>dffab390-a87b-4d08-93a3-8eedc7d24a7e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How did the breaking of the GPT-4 barrier in 2...</td>\n",
              "      <td>Yo, the breaking of the GPT-4 barrier in 2024 ...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2024, the breaking of the GPT-4 barrier sig...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.217965</td>\n",
              "      <td>e0f5c6c4-fb8e-411e-8399-95212616c92b</td>\n",
              "      <td>d10800e7-981e-4127-8641-d2f357f52723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the training efficiency of DeepSeek v...</td>\n",
              "      <td>Yo, check it! DeepSeek v3 is flexing some seri...</td>\n",
              "      <td>None</td>\n",
              "      <td>DeepSeek v3, a 685B parameter model, is one of...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7.265255</td>\n",
              "      <td>28816b79-8c83-49b7-90f3-dc5a99ee6a7f</td>\n",
              "      <td>cddb24bd-3a3c-4ad3-9f10-cebb03122d05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How has the development of models surpassing G...</td>\n",
              "      <td>Yo, the scene in 2024 is pretty wild! With 18 ...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2024, the development of models surpassing ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.281109</td>\n",
              "      <td>757f3ec9-c32f-4d24-a2f7-d02b88fdb499</td>\n",
              "      <td>2fa50d79-687c-4902-a08c-9e603a4bab64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do the ethics of AI and the gullibility of...</td>\n",
              "      <td>Yo, the ethics of AI and the gullibility of la...</td>\n",
              "      <td>None</td>\n",
              "      <td>The ethics of AI and the gullibility of langua...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.492535</td>\n",
              "      <td>bf0ac2b1-9738-4e30-8f8b-1b5d9e8c3b04</td>\n",
              "      <td>56695a52-31b0-4ce8-bd7f-f071a3649b82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What are the ethical concerns related to the g...</td>\n",
              "      <td>Yo, the ethical vibes around the gullibility o...</td>\n",
              "      <td>None</td>\n",
              "      <td>The ethical concerns related to the gullibilit...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.733880</td>\n",
              "      <td>d312a7f8-fd86-4df7-9d4d-6f9387a11412</td>\n",
              "      <td>32fe5cb2-0df4-4265-ad76-0487a7cbcb9f</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Why are Large Language Models (LLMs) considere...</td>\n",
              "      <td>LLMs are considered black boxes because, despi...</td>\n",
              "      <td>None</td>\n",
              "      <td>Large Language Models (LLMs) are considered bl...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7.673780</td>\n",
              "      <td>28659332-3637-4cee-9018-98d587575f5b</td>\n",
              "      <td>6d4eece2-36ce-45e1-b308-11ab50b14f34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are the ethical concerns associated with ...</td>\n",
              "      <td>Yo, the ethical vibes surrounding the gullibil...</td>\n",
              "      <td>None</td>\n",
              "      <td>The ethical concerns associated with the gulli...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>15.375634</td>\n",
              "      <td>2da5c189-eb9e-419b-9c93-e3247d9f154c</td>\n",
              "      <td>6b2701ac-8dd9-42bc-b4cf-92c410f43390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What role does Stanford Alpaca play in the dev...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>Stanford Alpaca is associated with the acceler...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.234988</td>\n",
              "      <td>1c5369b1-7b7e-476d-9d28-816ce1a360f6</td>\n",
              "      <td>4be48806-dee2-4810-9d3a-6a07e97d14ac</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Whaat are the key highlights from Simon Willis...</td>\n",
              "      <td>Yo, let’s break it down! In 2023, Simon Willis...</td>\n",
              "      <td>None</td>\n",
              "      <td>Simon Willison's Weblog highlights that 2023 w...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.724408</td>\n",
              "      <td>012f7bdc-73f0-4ad8-8a30-e89edbb0a0e7</td>\n",
              "      <td>2c0cd907-d3ed-452c-8bac-e5b3368a1980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What significant event related to AI ethics oc...</td>\n",
              "      <td>Yo, in September last year, there was a major ...</td>\n",
              "      <td>None</td>\n",
              "      <td>In September last year, the term 'prompt injec...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.863023</td>\n",
              "      <td>5719d5a1-4355-41b4-aabc-dd1c10357462</td>\n",
              "      <td>13719abd-f8cb-4d31-bf90-bb21317be497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Wut has Meta dun in the feeld of LLMs this yeer?</td>\n",
              "      <td>Yo, Meta's been making waves in the LLM scene ...</td>\n",
              "      <td>None</td>\n",
              "      <td>In February, Meta released Llama, and in July,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.654334</td>\n",
              "      <td>dcc029aa-596c-44f0-89c4-c54fa92184ea</td>\n",
              "      <td>e319edf2-130c-4379-ac89-62a205a21ad0</td>\n",
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
<<<<<<< HEAD
              "<ExperimentResults perfect-click-58>"
            ]
          },
          "execution_count": 43,
=======
              "<ExperimentResults artistic-airplane-98>"
            ]
          },
          "execution_count": 69,
>>>>>>> 1b963e52d261b1851b7c8927b6607b7fc9ab2f54
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    dope_rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        dope_or_nope_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"dope_chain\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C7migvlDPZT"
      },
      "source": [
        "#### 🏗️ Activity #3:\n",
        "\n",
        "Provide a screenshot of the difference between the two chains, and explain why you believe certain metrics changed in certain ways."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07ab3dc0790241bbb85a7f488a42ef8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7710c7377cbc4c30b55b28b4bc99e88f",
              "IPY_MODEL_41bdd49fab5f4826959d0d50663ff539",
              "IPY_MODEL_60168d85131d4afc99d55d61ab954ee6"
            ],
            "layout": "IPY_MODEL_9edf898aeeab40dda9b9475395776521"
          }
        },
        "095f680d37a3430fb82d223615662db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b44cb0f8e34446c8dde668a75d3d8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10df31709059484c99f102453d780473": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1160a44dc18e47b0890f70c40eaa7eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11c7f66acc1d45be9517d0addf49331e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "122b1bd1f0e9417a8dcb57d4eebe4d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0c233ad01604540a6c873f4a731982d",
              "IPY_MODEL_e9a01115c75b499884f7e0ef32e9e599",
              "IPY_MODEL_5faba4ad609448b2b49024add4ad3b8e"
            ],
            "layout": "IPY_MODEL_ef25efa751304e4699910f1fbc14345f"
          }
        },
        "158212a630f04cbd884c937f2f60f5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1d54ccd56494c4d831f71b416a1f880",
            "placeholder": "​",
            "style": "IPY_MODEL_530f696feefe499da08c6312047379b2",
            "value": " 20/? [01:43&lt;00:00,  5.25s/it]"
          }
        },
        "23863bc37a8645029934b8c106622c51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2508d229935744cbb5fc340222e2d660": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a0755d4476543feb4a64538e3e37213": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c20b539cd70b4ba99601ad1d69fd9cec",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6d681eeafa44d18b933a4c5dec88382",
            "value": 1
          }
        },
        "33f063017b7c4c7fa8cbafc89674350b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6864c81e2bcf459bbaf5acbb36bdfcbe",
              "IPY_MODEL_59d6e269eadf429a924f6f79bc8ba4ba",
              "IPY_MODEL_ca791fc471e34b9da2f9070fc1053c0f"
            ],
            "layout": "IPY_MODEL_8baf0ed3d0f743f294e07f2b5407e820"
          }
        },
        "3a8537e37fc14fd9b16ca0ceee4fede6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41bdd49fab5f4826959d0d50663ff539": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eb8b2e3262c45248708a2082c366f0a",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_095f680d37a3430fb82d223615662db5",
            "value": 64
          }
        },
        "530f696feefe499da08c6312047379b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59d6e269eadf429a924f6f79bc8ba4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_890e0dd7fa524ceca1e805cb6253ee71",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61b52ff459214129b8f7e6d67b192b78",
            "value": 20
          }
        },
        "5ab5f08afa5841709aedb2f78a52a11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c2fda99d4204d85b1bf7ad354fd58d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5faba4ad609448b2b49024add4ad3b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_849b5c95008541d49f1ceedf0a59ac60",
            "placeholder": "​",
            "style": "IPY_MODEL_f3665a86662746c4ac7cb0796604781d",
            "value": " 20/? [01:27&lt;00:00,  6.45s/it]"
          }
        },
        "60168d85131d4afc99d55d61ab954ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8537e37fc14fd9b16ca0ceee4fede6",
            "placeholder": "​",
            "style": "IPY_MODEL_1160a44dc18e47b0890f70c40eaa7eb0",
            "value": " 61/64 [00:02&lt;00:00, 23.36it/s]"
          }
        },
        "61b52ff459214129b8f7e6d67b192b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6864c81e2bcf459bbaf5acbb36bdfcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10df31709059484c99f102453d780473",
            "placeholder": "​",
            "style": "IPY_MODEL_2508d229935744cbb5fc340222e2d660",
            "value": "Generating: 100%"
          }
        },
        "6eb8b2e3262c45248708a2082c366f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7710c7377cbc4c30b55b28b4bc99e88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c2fda99d4204d85b1bf7ad354fd58d4",
            "placeholder": "​",
            "style": "IPY_MODEL_93cd4d35c5fd41f5904ca1d52d1f52a8",
            "value": "embedding nodes:  95%"
          }
        },
        "7cb241365f604419af454c1c28de197a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7dce19ac55264f2b88a0e4730e55867b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddffd834e09940a4bd3874c3f39b4e21",
            "placeholder": "​",
            "style": "IPY_MODEL_ef63c3b2d51e452da03cdae5d9b034be",
            "value": ""
          }
        },
        "849b5c95008541d49f1ceedf0a59ac60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "890e0dd7fa524ceca1e805cb6253ee71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8baf0ed3d0f743f294e07f2b5407e820": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93cd4d35c5fd41f5904ca1d52d1f52a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cf586576ff44dba86ba2eb389593c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9edf898aeeab40dda9b9475395776521": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a6d681eeafa44d18b933a4c5dec88382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf8dcc0895054529af356da401c513f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dce19ac55264f2b88a0e4730e55867b",
              "IPY_MODEL_2a0755d4476543feb4a64538e3e37213",
              "IPY_MODEL_158212a630f04cbd884c937f2f60f5c8"
            ],
            "layout": "IPY_MODEL_11c7f66acc1d45be9517d0addf49331e"
          }
        },
        "c20b539cd70b4ba99601ad1d69fd9cec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ca791fc471e34b9da2f9070fc1053c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23863bc37a8645029934b8c106622c51",
            "placeholder": "​",
            "style": "IPY_MODEL_5ab5f08afa5841709aedb2f78a52a11c",
            "value": " 20/20 [00:52&lt;00:00,  4.50s/it]"
          }
        },
        "d1d54ccd56494c4d831f71b416a1f880": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddffd834e09940a4bd3874c3f39b4e21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c233ad01604540a6c873f4a731982d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b44cb0f8e34446c8dde668a75d3d8ad",
            "placeholder": "​",
            "style": "IPY_MODEL_edaac6587b2d4bd5be52b89bb097f99f",
            "value": ""
          }
        },
        "e9a01115c75b499884f7e0ef32e9e599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb241365f604419af454c1c28de197a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cf586576ff44dba86ba2eb389593c61",
            "value": 1
          }
        },
        "edaac6587b2d4bd5be52b89bb097f99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef25efa751304e4699910f1fbc14345f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef63c3b2d51e452da03cdae5d9b034be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3665a86662746c4ac7cb0796604781d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
